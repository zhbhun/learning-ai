
## 数据

- [Open-LLM-datasets](https://github.com/dsdanielpark/open-llm-datasets)
- [基于ChatGPT输出的10个精选数据集](https://www.atyun.com/56192.html)
- [A Complete Guide to LLM Datasets | Machine Learning Dataset](https://medium.com/@wangshally11/a-complete-guide-to-llm-datasets-machine-learning-dataset-2865bbc7332c)
- [Open-Sourced Training Datasets for Large Language Models (LLMs)](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)
- [LLMDataHub](https://github.com/Zjh-819/LLMDataHub) - A quick guide (especially) for trending instruction finetuning datasets
- [LLM-数据集收集](https://zhuanlan.zhihu.com/p/648412136)

### 书籍

- [bookcorpus](https://huggingface.co/datasets/bookcorpus) - 小语言模型如 GPT-2 常用的数据集，包括超过 11000 本电子书，主要包括小说和传记。
- [Gutenberg](https://www.gutenberg.org/) - 有 70000 本书，包括小说、散文、戏剧等作品，是目前最大的开源书籍语料库之一。

### 故事

- https://huggingface.co/datasets/roneneldan/TinyStories

  - [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)
  - [TinyStories: A Tiny Dataset with Big Impact](https://satwikgawand.medium.com/tinystories-a-tiny-dataset-with-big-impact-f788d546b6f)

- [A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation](https://ar5iv.labs.arxiv.org/html/2001.05139?_immersive_translate_auto_translate=1)
- [Awesome-Story-Generation](https://github.com/yingpengma/Awesome-Story-Generation?tab=readme-ov-file)

### 网络

- [CommonCrawl](https://commoncrawl.org/) - 这个是目前最大的开源网络爬虫数据库，不过这个数据包含了大量脏数据，所以目前常用的四个数据库是 C4、CC-Stories、CC-News 和 RealNews。另外还有两个基于 CommonCrawl 提取的新闻语料库 REALNEWS 和 CC-News。
- [openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) - An open-source replication of the WebText dataset from OpenAI, that was used to train GPT-2.
- Reddit - http://pushshift.io
- Wikipedia

### 代码

- Github
- StackOverflow

### 对话

- [ultrachat](https://huggingface.co/datasets/stingning/ultrachat) - An open-source, large-scale, and multi-round dialogue data powered by Turbo APIs. 

### 中文

- https://github.com/BAAI-Zlab/COIG - https://huggingface.co/datasets/BAAI/COIG - 	Chinese Open Instruction Generalist project
- https://github.com/ymcui/Chinese-LLaMA-Alpaca - Alpaca 数据集中文翻译（ChatGPT 辅助翻译）
- https://huggingface.co/datasets/BelleGroup/train_2M_CN - BELLE 项目的中文数据集（ChatGPT 生成）
- https://huggingface.co/datasets/JosephusCheung/GuanacoDataset - Guannaco 模型的对话数据集
- https://huggingface.co/datasets/suolyer/webqa - 中文网络问答
- https://github.com/CLUEbenchmark/pCLUE - 基于提示的大规模预训练数据集，用于多任务学习和零样本学习
- https://github.com/CVI-SZU/Linly/blob/main/instructions/README.md
- https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/data/README.md
- [CBook-150K](https://github.com/FudanNLPLAB/CBook-150K)
- [LLM-SFT](https://github.com/yongzhuo/LLM-SFT) - 中文大模型微调(LLM-SFT), 数学指令数据集
- [国产ChatGPT「套壳」的秘密，现在被找到了](https://wallstreetcn.com/articles/3689782)
- [中文指令微调数据集（长期更新）](https://zhuanlan.zhihu.com/p/631640097)

## 生成

- [如何用chatgpt生成数据集](https://www.chatairc.com/32511/)

  收集种子数据 =》选择模型和参数 =》微调模型 =》生成数据集 =》过滤和清洗数据 =》划分数据集 =》导出数据集

- [如何制作自己的代码生成数据集(代码生成 数据集)](https://bitcloudcoin.com/code-generation-dataset/)
- [如何利用 ChatGPT 进行自动数据清理和预处理](https://www.mvrlink.com/chatgpt-for-automatic-data-cleaning-and-preprocessing/)