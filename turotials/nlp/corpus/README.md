
## 数据

### 书籍

- [bookcorpus](https://huggingface.co/datasets/bookcorpus) - 小语言模型如 GPT-2 常用的数据集，包括超过 11000 本电子书，主要包括小说和传记。
- [Gutenberg](https://www.gutenberg.org/) - 有 70000 本书，包括小说、散文、戏剧等作品，是目前最大的开源书籍语料库之一。

### 网络

- [CommonCrawl](https://commoncrawl.org/) - 这个是目前最大的开源网络爬虫数据库，不过这个数据包含了大量脏数据，所以目前常用的四个数据库是 C4、CC-Stories、CC-News 和 RealNews。另外还有两个基于 CommonCrawl 提取的新闻语料库 REALNEWS 和 CC-News。
- [openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) - An open-source replication of the WebText dataset from OpenAI, that was used to train GPT-2.
- Reddit - http://pushshift.io
- Wikipedia

### 代码

- Github
- StackOverflow

## 生成

- [如何用chatgpt生成数据集](https://www.chatairc.com/32511/)

  收集种子数据 =》选择模型和参数 =》微调模型 =》生成数据集 =》过滤和清洗数据 =》划分数据集 =》导出数据集